{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c37d4c9",
   "metadata": {},
   "source": [
    "# Best Model and Parameter Selection for Digits Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550b1066",
   "metadata": {},
   "source": [
    "## Imports and data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6492d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b3fc717",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DESCR', 'data', 'feature_names', 'frame', 'images', 'target', 'target_names']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits = load_digits()\n",
    "dir(digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5e91a0",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351965ce",
   "metadata": {},
   "source": [
    "Not needed because the models also take nparrays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4c9227",
   "metadata": {},
   "source": [
    "## Defining models array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2629ab9",
   "metadata": {},
   "source": [
    ">The following array is generated by ChatGPT as he can do the 'guessing' task better. This array is large and takes a lot of CPU if run locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "504d2c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "\n",
    "    'svm': {\n",
    "        'model': SVC(),\n",
    "        'params': {\n",
    "            'C': [0.1, 1, 10],\n",
    "            'kernel': ['linear', 'rbf', 'poly'],\n",
    "            'gamma': ['scale', 0.1, 1]\n",
    "        }\n",
    "    },\n",
    "\n",
    "    'LogisticRegression': {\n",
    "        'model': LogisticRegression(max_iter=1000),\n",
    "        'params': {\n",
    "            'C': [0.1, 1, 10],\n",
    "            'penalty': ['l2'],\n",
    "            'solver': ['lbfgs']\n",
    "        }\n",
    "    },\n",
    "\n",
    "    'MultinomialNB': {\n",
    "        'model': MultinomialNB(),\n",
    "        'params': {\n",
    "            'alpha': [0.1, 0.5, 1.0]\n",
    "        }\n",
    "    },\n",
    "\n",
    "    'GaussianNB': {\n",
    "        'model': GaussianNB(),\n",
    "        'params': {\n",
    "            'var_smoothing': [1e-9, 1e-8, 1e-7]\n",
    "        }\n",
    "    },\n",
    "\n",
    "    'DecisionTree': {\n",
    "        'model': DecisionTreeClassifier(),\n",
    "        'params': {\n",
    "            'max_depth': [None, 5, 10, 20],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'criterion': ['gini', 'entropy']\n",
    "        }\n",
    "    },\n",
    "\n",
    "    'RandomForest': {\n",
    "        'model': RandomForestClassifier(),\n",
    "        'params': {\n",
    "            'n_estimators': [100, 200],\n",
    "            'max_depth': [None, 10, 20],\n",
    "            'min_samples_split': [2, 5],\n",
    "            'max_features': ['sqrt', 'log2']\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd706afb",
   "metadata": {},
   "source": [
    "## Training Every possible model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be00403",
   "metadata": {},
   "source": [
    "And seeing the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b5adc7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HC\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "c:\\Users\\HC\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "c:\\Users\\HC\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "c:\\Users\\HC\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "c:\\Users\\HC\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "c:\\Users\\HC\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "c:\\Users\\HC\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "c:\\Users\\HC\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "c:\\Users\\HC\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "c:\\Users\\HC\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "c:\\Users\\HC\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "c:\\Users\\HC\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "c:\\Users\\HC\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "c:\\Users\\HC\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "c:\\Users\\HC\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "c:\\Users\\HC\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Grid Search CV tries every combination for all parameters for ONE MODEL. For more than one models, we still \n",
    "# need a for loop\n",
    "\n",
    "scores = []\n",
    "\n",
    "for model_name, model_dict in models.items():\n",
    "    grid_search_obj = GridSearchCV(model_dict['model'], model_dict['params'], cv=5, return_train_score=False)\n",
    "    grid_search_obj.fit(digits.data, digits.target)\n",
    "    scores.append({\n",
    "        'model' : model_name,\n",
    "        'best parameters' : grid_search_obj.best_params_,\n",
    "        'best score' : grid_search_obj.best_score_\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "496d0647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>best parameters</th>\n",
       "      <th>best score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>svm</td>\n",
       "      <td>{'C': 10, 'gamma': 'scale', 'kernel': 'rbf'}</td>\n",
       "      <td>0.973850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>{'C': 0.1, 'penalty': 'l2', 'solver': 'lbfgs'}</td>\n",
       "      <td>0.918217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>{'alpha': 0.1}</td>\n",
       "      <td>0.870907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>{'var_smoothing': 1e-07}</td>\n",
       "      <td>0.832518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 20, 'min...</td>\n",
       "      <td>0.814708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>{'max_depth': 20, 'max_features': 'log2', 'min...</td>\n",
       "      <td>0.943254</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                model                                    best parameters  \\\n",
       "0                 svm       {'C': 10, 'gamma': 'scale', 'kernel': 'rbf'}   \n",
       "1  LogisticRegression     {'C': 0.1, 'penalty': 'l2', 'solver': 'lbfgs'}   \n",
       "2       MultinomialNB                                     {'alpha': 0.1}   \n",
       "3          GaussianNB                           {'var_smoothing': 1e-07}   \n",
       "4        DecisionTree  {'criterion': 'entropy', 'max_depth': 20, 'min...   \n",
       "5        RandomForest  {'max_depth': 20, 'max_features': 'log2', 'min...   \n",
       "\n",
       "   best score  \n",
       "0    0.973850  \n",
       "1    0.918217  \n",
       "2    0.870907  \n",
       "3    0.832518  \n",
       "4    0.814708  \n",
       "5    0.943254  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = pd.DataFrame(scores)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd566dae",
   "metadata": {},
   "source": [
    "**For me, the winner is SVM with listed parameters.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
